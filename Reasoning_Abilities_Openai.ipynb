{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmedayaz1210/Reasoning_Abilities_Openai/blob/main/Reasoning_Abilities_Openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries"
      ],
      "metadata": {
        "id": "Pt_SMGlvocqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai groq"
      ],
      "metadata": {
        "id": "eKvKwO8XAnPt",
        "outputId": "b790f09c-b33f-4953-fc33-d0a4c81822cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.50.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.50.0-py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.9/378.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai, groq\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Groq, OpenRouter, & OpenAI clients"
      ],
      "metadata": {
        "id": "GjcgEeFaof1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHbjDU_L__Vd"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "import json\n",
        "from typing import List, Dict, Any, Callable\n",
        "import ast\n",
        "import io\n",
        "import sys\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "openrouter_api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "os.environ['OPENROUTER_API_KEY'] = openrouter_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    base_url=\"https://api.openai.com/v1\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions to easily query and compare responses from OpenAI, Groq, and OpenRouter"
      ],
      "metadata": {
        "id": "AZF6uLpooj-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(client, prompt, openai_model=\"gpt-4\", json_mode=False):\n",
        "\n",
        "    if client == \"openai\":\n",
        "\n",
        "        kwargs = {\n",
        "            \"model\": openai_model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "\n",
        "        if json_mode:\n",
        "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        response = openai_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    elif client == \"groq\":\n",
        "\n",
        "        try:\n",
        "            models = [\"llama-3.1-8b-instant\", \"llama-3.1-70b-versatile\", \"llama3-70b-8192\", \"llama3-8b-8192\", \"gemma2-9b-it\"]\n",
        "\n",
        "            for model in models:\n",
        "\n",
        "                try:\n",
        "                    kwargs = {\n",
        "                        \"model\": model,\n",
        "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "                    }\n",
        "                    if json_mode:\n",
        "                        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "                    response = groq_client.chat.completions.create(**kwargs)\n",
        "\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "            kwargs = {\n",
        "                \"model\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "            }\n",
        "\n",
        "            if json_mode:\n",
        "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "            response = openrouter_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid client: {client}\")\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def evaluate_responses(prompt, reasoning_prompt=False, openai_model=\"o1-preview\"):\n",
        "\n",
        "    if reasoning_prompt:\n",
        "        prompt = f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
        "\n",
        "    openai_response = get_llm_response(\"openai\", prompt, openai_model)\n",
        "    groq_response = get_llm_response(\"groq\", prompt)\n",
        "\n",
        "    print(f\"OpenAI Response: {openai_response}\")\n",
        "    print(f\"\\n\\nGroq Response: {groq_response}\")"
      ],
      "metadata": {
        "id": "BNYyDCNuAhbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1=\"How many r's are in the word 'strawberry?\"\n",
        "evaluate_responses(prompt1, openai_model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "j9V03RXNBd6j",
        "outputId": "2fb53359-169d-41df-ee60-5073d65a0ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: There are 2 r's in the word 'strawberry'.\n",
            "\n",
            "\n",
            "Groq Response: There is one 'r' in the word 'strawberry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = \"9.9 or 9.11 which one is bigger?\"\n",
        "evaluate_responses(prompt2, openai_model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "wYaiRrHwbwMT",
        "outputId": "1e6e2a38-169f-432f-f3d4-4d69c7cee394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: 9.11 is bigger.\n",
            "\n",
            "\n",
            "Groq Response: The numbers 9.9 and 9.11 are both decimals. To determine which one is bigger, we can look at the numbers after the decimal points.\n",
            "\n",
            "In 9.9, the decimal point is exactly at 9. In 9.11, the decimal point is above 9. So, comparing the numbers, 9.11 is larger than 9.9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_prompt=\"Let's first understand the problem and devise a plan to solve it. Then, let's carry out the plan and solve the problem step by step.\"\n",
        "evaluate_responses(prompt1, reasoning_prompt, openai_model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "Mq1YkfFeCRl9",
        "outputId": "6cfaf002-dcad-44aa-8347-7bac9ddd880b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: Understanding the Problem:\n",
            "This problem is asking us to count how many times the letter \"r\" appears in the word \"strawberry\".\n",
            "\n",
            "Devising a Plan:\n",
            "The plan will be to look at each letter in the word \"strawberry\" one by one and count the number of times the letter \"r\" appears.\n",
            "\n",
            "Carrying Out the Plan:\n",
            "Looking at the word \"strawberry\", the letter \"r\" appears in the 4th position, the 9th position, and the 10th position. So we can see the letter \"r\" appears 3 times.\n",
            "\n",
            "Answer:\n",
            "Therefore, there are 3 \"r's\" in the word \"strawberry\".\n",
            "\n",
            "\n",
            "Groq Response: To find the number of R's in the word 'strawberry', we can devise a simple plan to solve the problem.\n",
            "\n",
            "**Plan:** We will write out the word 'strawberry' and count the number of times the letter R appears.\n",
            "\n",
            "**Step 1:** Write out the word 'strawberry'.\n",
            "\n",
            "The word is: s-t-r-a-w-b-e-r-r-y\n",
            "\n",
            "**Step 2:** Identify the letter R in the word.\n",
            "\n",
            "We can see that the letter R appears twice in the word.\n",
            "\n",
            "**Step 3:** Verify the count.\n",
            "\n",
            "Counting the number of times the letter R appears, we find that there are indeed 2 R's in the word 'strawberry'.\n",
            "\n",
            "Therefore, the final answer is: There are 2 R's in the word 'strawberry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgyXx8M9C0RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Architecture"
      ],
      "metadata": {
        "id": "0w8IlZq49PAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![](https://mermaid.ink/img/pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW?type=png)](https://mermaid.live/edit#pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW)\n",
        "\n",
        "\n",
        "![agent_architecture_v2](https://github.com/user-attachments/assets/a65b6db9-bef1-4579-aed3-01444ce40544)"
      ],
      "metadata": {
        "id": "RxpUp2KED9hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To create our AI Agent, we will define the following functions:\n",
        "\n",
        "1. **Planner:** This function takes a user's query and breaks it down into smaller, manageable subtasks. It returns these subtasks as a list, where each one is either a reasoning task or a code generation task.\n",
        "\n",
        "2. **Reasoner:** This function provides reasoning on how to complete a specific subtask, considering both the overall query and the results of any previous subtasks. It returns a short explanation on how to proceed with the current subtask.\n",
        "\n",
        "3. **Actioner:** Based on the reasoning provided for a subtask, this function decides whether the next step requires generating code or more reasoning. It then returns the chosen action and any necessary details to perform it.\n",
        "\n",
        "4. **Evaluator:** This function checks if the result of the current subtask is reasonable and aligns with the overall goal. It returns an evaluation of the result and indicates whether the subtask needs to be retried.\n",
        "\n",
        "5. **generate_and_execute_code:** This function generates and executes Python code based on a given prompt and memory of previous steps. It returns both the generated code and its execution result.\n",
        "\n",
        "6. **executor:** Depending on the action decided by the \"actioner,\" this function either generates and executes code or returns reasoning. It handles the execution of tasks based on the action type.\n",
        "\n",
        "7. **final_answer_extractor:** After all subtasks are completed, this function gathers the results from previous steps to extract and provide the final answer to the user's query.\n",
        "\n",
        "8. **autonomous_agent:** This is the main function that coordinates the process of answering the user's query. It manages the entire sequence of planning, reasoning, action, evaluation, and final answer extraction to produce a complete response."
      ],
      "metadata": {
        "id": "goGb1KUVmVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def planner(user_query) -> List[str]:\n",
        "  prompt = f\"\"\" Given the user's query: '{user_query}', break down the query into as few subtasks as possible in order to answer the question.\n",
        "\n",
        "  Each subtask should be either a reasoning task or a code generation task. Never duplicate a task.\n",
        "  Here are the only 2 actions that can be taken for each subtask:\n",
        "    - generate_code: This action involves generating Python code and executing it in order to make a calculation or verification.\n",
        "    - reasoning: This action involves providing reasoning for what to do to complete the subtask.\n",
        "\n",
        "  Each subtask should begin with either \"reasoning\" or \"generate_code\".\n",
        "\n",
        "  Keep in mind the overall goal of answering the user's query throughout the planning process.\n",
        "\n",
        "  Return the result as a JSON list of strings, where each string is a subtask.\n",
        "\n",
        "  Here is an example JSON response:\n",
        "  {{\n",
        "    \"subtasks\": [\"Subtask1\", \"Subtask2\", \"Subtask3\"]\n",
        "  }}\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  response =json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "  return response[\"subtasks\"]"
      ],
      "metadata": {
        "id": "BpZ1hRaiD-hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many r's are in the word 'strawberry?\"\n",
        "subtasks = planner(query)\n",
        "subtasks"
      ],
      "metadata": {
        "id": "MUMjfWwcK7ig",
        "outputId": "b9a70dbc-84c9-4b76-dfad-09b8d2ad1319",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"reasoning: Identify the word that the user is referring to as 'strawberry'.\",\n",
              " \"generate_code: Write a Python function to count the occurrences of the letter 'r' in the word 'strawberry'.\",\n",
              " 'reasoning: Execute the function to obtain the count and return the result to the user.',\n",
              " \"generate_code: Implement error handling to account for cases where the user's query is not referring to a word.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reasoner(user_query: str, subtasks: List[str], current_subtask: str) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   Here are all the subtasks to complete in order to answer the user's query:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   - Provide concise reasoning on how to execute the current subtask, considering previous results and subtasks\n",
        "   - Prioritize explicit details over assumed patterns\n",
        "   - Avoid unnecessary complications in problem-solving\n",
        "\n",
        "   Return the result as a JSON object with 'reasoning' as a key.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
        "   }}\n",
        "   \"\"\"\n",
        "   response =json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"reasoning\"]"
      ],
      "metadata": {
        "id": "BWjY2S2mxh3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoner_output= reasoner(query, subtasks, subtasks[1])"
      ],
      "metadata": {
        "id": "1BFyiCpRxh6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The reasoning for this subtask is:\n",
        "   <reasoning>\n",
        "       {reasoning}\n",
        "   </reasoning>\n",
        "\n",
        "   Determine the most appropriate action to take:\n",
        "       - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
        "       - If the task requires reasoning without code or calculations, use the 'reasoning' action.\n",
        "\n",
        "   Consider the overall goal and previous results when determining the action.\n",
        "\n",
        "   Return the result as a JSON object with 'action' and 'parameters' keys.  The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
        "\n",
        "   Example JSON responses:\n",
        "\n",
        "   {{\n",
        "       \"action\": \"generate_code\",\n",
        "       \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
        "   }}\n",
        "\n",
        "   {{\n",
        "       \"action\": \"reasoning\",\n",
        "       \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
        "   }}\n",
        "   \"\"\"\n",
        "   response =json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n"
      ],
      "metadata": {
        "id": "IrJOGaqc-Hvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actioner_output = actioner(query, subtasks, subtasks[1], reasoner_output)"
      ],
      "metadata": {
        "id": "QDUfI3tM-Izu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_execute_code(prompt: str, user_query: str) -> Dict[str, Any]:\n",
        "   code_generation_prompt = f\"\"\"\n",
        "\n",
        "   Generate Python code to implement the following task: '{prompt}'\n",
        "\n",
        "   Here is the overall goal of answering the user's query: '{user_query}'\n",
        "\n",
        "   Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
        "\n",
        "\n",
        "\n",
        "   Here are the guidelines for generating the code:\n",
        "       - Return only the Python code, without any explanations or markdown formatting.\n",
        "       - The code should always print or return a value\n",
        "       - Don't include any backticks or code blocks in your response. Do not include ```python or ``` in your response, just give me the code.\n",
        "       - Do not ever use the input() function in your code, use defined values instead.\n",
        "       - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
        "       - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
        "       - Don't ever provide the execution result in your response, just give me the code.\n",
        "       - If your code needs to import any libraries, do it within the code itself.\n",
        "       - The code should be self-contained and ready to execute on its own.\n",
        "       - Prioritize explicit details over assumed patterns\n",
        "       - Avoid unnecessary complications in problem-solving\n",
        "   \"\"\"\n",
        "\n",
        "   generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "\n",
        "   print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "   old_stdout = sys.stdout\n",
        "   sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "   exec(generated_code)\n",
        "\n",
        "   sys.stdout = old_stdout\n",
        "   output = buffer.getvalue()\n",
        "\n",
        "   print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "   return {\n",
        "       \"generated_code\": generated_code,\n",
        "       \"execution_result\": output.strip()\n",
        "   }\n",
        "\n",
        "\n",
        "def executor(action: str, parameters: Dict[str, Any], user_query: str) -> Any:\n",
        "   if action == \"generate_code\":\n",
        "       print(f\"Generating code for: {parameters['prompt']}\")\n",
        "       return generate_and_execute_code(parameters[\"prompt\"], user_query)\n",
        "   elif action == \"reasoning\":\n",
        "       return parameters[\"prompt\"]\n",
        "   else:\n",
        "       return f\"Action '{action}' not implemented\"\n"
      ],
      "metadata": {
        "id": "rMcV07zx-JMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "executor(\"generate_code\", parameters=actioner_output['parameters'], user_query = query)"
      ],
      "metadata": {
        "id": "3-5has84AB_C",
        "outputId": "079e51b2-5a13-4b40-870c-fa9695b5f852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating code for: Write a Python function to count the occurrences of the letter 'r' in the word 'strawberry'.\n",
            "\n",
            "\n",
            "Generated Code: start|def count_r_in_word():\n",
            "    word = 'strawberry'\n",
            "    word = word.lower()\n",
            "    count = word.count('r')\n",
            "    print(count)\n",
            "\n",
            "count_r_in_word()|END\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "***** Execution Result: |start|3|end| *****\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'generated_code': \"def count_r_in_word():\\n    word = 'strawberry'\\n    word = word.lower()\\n    count = word.count('r')\\n    print(count)\\n\\ncount_r_in_word()\",\n",
              " 'execution_result': '3'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def planner(user_query: str) -> List[str]:\n",
        "   prompt = f\"\"\"Given the user's query: '{user_query}', break down the query into as few subtasks as possible in order to anser the question.\n",
        "   Each subtask is either a calculation or reasoning step. Never duplicate a task.\n",
        "\n",
        "   Here are the only 2 actions that can be taken for each subtask:\n",
        "       - generate_code: This action involves generating Python code and executing it in order to make a calculation or verification.\n",
        "       - reasoning: This action involves providing reasoning for what to do to complete the subtask.\n",
        "\n",
        "   Each subtask should begin with either \"reasoning\" or \"generate_code\".\n",
        "\n",
        "\n",
        "   Keep in mind the overall goal of answering the user's query throughout the planning process.\n",
        "\n",
        "   Return the result as a JSON list of strings, where each string is a subtask.\n",
        "\n",
        "   Here is an example JSON response:\n",
        "\n",
        "   {{\n",
        "       \"subtasks\": [\"Subtask 1\", \"Subtask 2\", \"Subtask 3\"]\n",
        "   }}\n",
        "   \"\"\"\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   print(response)\n",
        "   return response[\"subtasks\"]\n",
        "\n",
        "\n",
        "def reasoner(user_query: str, subtasks: List[str], current_subtask: str, memory: List[Dict[str, Any]]) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   Here are all the subtasks to complete in order to answer the user's query:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   - Provide concise reasoning on how to execute the current subtask, considering previous results.\n",
        "   - Prioritize explicit details over assumed patterns\n",
        "   - Avoid unnecessary complications in problem-solving\n",
        "\n",
        "   Return the result as a JSON object with 'reasoning' as a key.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"reasoning\"]\n",
        "\n",
        "\n",
        "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The reasoning for this subtask is:\n",
        "   <reasoning>\n",
        "       {reasoning}\n",
        "   </reasoning>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   Determine the most appropriate action to take:\n",
        "       - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
        "       - If the task requires reasoning without code or calculations, use the 'reasoning' action.\n",
        "\n",
        "   Consider the overall goal and previous results when determining the action.\n",
        "\n",
        "   Return the result as a JSON object with 'action' and 'parameters' keys.  The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
        "\n",
        "   Example JSON responses:\n",
        "\n",
        "   {{\n",
        "       \"action\": \"generate_code\",\n",
        "       \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
        "   }}\n",
        "\n",
        "   {{\n",
        "       \"action\": \"reasoning\",\n",
        "       \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n",
        "\n",
        "\n",
        "def evaluator(user_query: str, subtasks: List[str], current_subtask: str, action_info: Dict[str, Any], execution_result: Dict[str, Any], memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks to complete to answer the user's query are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The result of the current subtask is:\n",
        "   <result>\n",
        "       {action_info}\n",
        "   </result>\n",
        "\n",
        "   The execution result of the current subtask is:\n",
        "   <execution_result>\n",
        "       {execution_result}\n",
        "   </execution_result>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "\n",
        "   Evaluate if the result is a reasonable answer for the current subtask, and makes sense in the context of the overall query.\n",
        "\n",
        "   Return a JSON object with 'evaluation' (string) and 'retry' (boolean) keys.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"evaluation\": \"The result is a reasonable answer for the current subtask.\",\n",
        "       \"retry\": false\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n",
        "\n",
        "\n",
        "def final_answer_extractor(user_query: str, subtasks: List[str], memory: List[Dict[str, Any]]) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks completed to answer the user's query are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The memory of the thought process (short-term memory) is:\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   Extract the final answer that directly addresses the user's query, from the memory.\n",
        "   Provide only the essential information without unnecessary explanations.\n",
        "\n",
        "   Return a JSON object with 'finalAnswer' as a key.\n",
        "\n",
        "   Here is an example JSON response:\n",
        "   {{\n",
        "       \"finalAnswer\": \"The final answer to the user's query, addressing all aspects of the question, based on the memory provided\",\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"finalAnswer\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_and_execute_code(prompt: str, user_query: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   code_generation_prompt = f\"\"\"\n",
        "\n",
        "   Generate Python code to implement the following task: '{prompt}'\n",
        "\n",
        "   Here is the overall goal of answering the user's query: '{user_query}'\n",
        "\n",
        "   Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "\n",
        "\n",
        "   Here are the guidelines for generating the code:\n",
        "       - Return only the Python code, without any explanations or markdown formatting.\n",
        "       - The code should always print or return a value\n",
        "       - Don't include any backticks or code blocks in your response. Do not include ```python or ``` in your response, just give me the code.\n",
        "       - Do not ever use the input() function in your code, use defined values instead.\n",
        "       - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
        "       - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
        "       - Don't ever provide the execution result in your response, just give me the code.\n",
        "       - If your code needs to import any libraries, do it within the code itself.\n",
        "       - The code should be self-contained and ready to execute on its own.\n",
        "       - Prioritize explicit details over assumed patterns\n",
        "       - Avoid unnecessary complications in problem-solving\n",
        "   \"\"\"\n",
        "\n",
        "   generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "\n",
        "   print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "   old_stdout = sys.stdout\n",
        "   sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "   exec(generated_code)\n",
        "\n",
        "   sys.stdout = old_stdout\n",
        "   output = buffer.getvalue()\n",
        "\n",
        "   print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "   return {\n",
        "       \"generated_code\": generated_code,\n",
        "       \"execution_result\": output.strip()\n",
        "   }\n",
        "\n",
        "\n",
        "def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
        "   if action == \"generate_code\":\n",
        "       print(f\"Generating code for: {parameters['prompt']}\")\n",
        "       return generate_and_execute_code(parameters[\"prompt\"], user_query, memory)\n",
        "   elif action == \"reasoning\":\n",
        "       return parameters[\"prompt\"]\n",
        "   else:\n",
        "       return f\"Action '{action}' not implemented\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def autonomous_agent(user_query: str) -> List[Dict[str, Any]]:\n",
        "   memory = []\n",
        "   subtasks = planner(user_query)\n",
        "\n",
        "   print(\"User Query:\", user_query)\n",
        "   print(f\"Subtasks: {subtasks}\")\n",
        "\n",
        "   for subtask in subtasks:\n",
        "       max_retries = 1\n",
        "       for attempt in range(max_retries):\n",
        "\n",
        "           reasoning = reasoner(user_query, subtasks, subtask, memory)\n",
        "           action_info = actioner(user_query, subtasks, subtask, reasoning, memory)\n",
        "\n",
        "\n",
        "           print(f\"\\n\\n ****** Action Info: {action_info} ****** \\n\\n\")\n",
        "\n",
        "           execution_result = executor(action_info[\"action\"], action_info[\"parameters\"], user_query, memory)\n",
        "\n",
        "           print(f\"\\n\\n ****** Execution Result: {execution_result} ****** \\n\\n\")\n",
        "           evaluation = evaluator(user_query, subtasks, subtask, action_info, execution_result, memory)\n",
        "\n",
        "           step = {\n",
        "               \"subtask\": subtask,\n",
        "               \"reasoning\": reasoning,\n",
        "               \"action\": action_info,\n",
        "               \"evaluation\": evaluation\n",
        "           }\n",
        "           memory.append(step)\n",
        "\n",
        "           print(f\"\\n\\nSTEP: {step}\\n\\n\")\n",
        "\n",
        "           if not evaluation[\"retry\"]:\n",
        "               break\n",
        "\n",
        "           if attempt == max_retries - 1:\n",
        "               print(f\"Max retries reached for subtask: {subtask}\")\n",
        "\n",
        "   final_answer = final_answer_extractor(user_query, subtasks, memory)\n",
        "   return final_answer"
      ],
      "metadata": {
        "id": "8fUAQsNGAC5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Who is the surgeon to the boy?\"\n",
        "result = get_llm_response(\"openai\", query)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "YUki69PtCpBI",
        "outputId": "127f60f4-5cbe-4ed2-deb4-ea0e2acd6192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The surgeon is the boy's father.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = autonomous_agent(query)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "WvOCdgB6DHxq",
        "outputId": "37772a30-0ecf-4dcf-e620-0b8a66cb7713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'subtasks': ['reasoning: Identify the key elements of the query to determine the question being asked.', 'reasoning: Understand the relationship between the surgeon and the boy to answer the question.', 'reasoning: Determine the type of relationship that makes the surgeon unable to operate on the boy.', 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.', 'reasoning: Filter the list to find relationships that would be relevant to the question.', 'reasoning: Select the most relevant relationship to answer the question.', 'reasoning: Use this relationship to determine who the surgeon is to the boy.']}\n",
            "User Query: The surgeon, who is the boy's father, says, 'I can't operate on this boy, he's my son!' Who is the surgeon to the boy?\n",
            "Subtasks: ['reasoning: Identify the key elements of the query to determine the question being asked.', 'reasoning: Understand the relationship between the surgeon and the boy to answer the question.', 'reasoning: Determine the type of relationship that makes the surgeon unable to operate on the boy.', 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.', 'reasoning: Filter the list to find relationships that would be relevant to the question.', 'reasoning: Select the most relevant relationship to answer the question.', 'reasoning: Use this relationship to determine who the surgeon is to the boy.']\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Read the query carefully to identify the key elements such as the surgeon, the boy, and the action of operating. Then, ask yourself what aspect of the situation is highlighted in the question 'Who is the surgeon to the boy?'\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Read the query carefully to identify the key elements such as the surgeon, the boy, and the action of operating. Then, ask yourself what aspect of the situation is highlighted in the question 'Who is the surgeon to the boy?' ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Identify the key elements of the query to determine the question being asked.', 'reasoning': \"Read the query carefully to identify the key elements such as the surgeon, the boy, and the action of operating. Then, ask yourself what aspect of the situation is highlighted in the question 'Who is the surgeon to the boy?'\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Read the query carefully to identify the key elements such as the surgeon, the boy, and the action of operating. Then, ask yourself what aspect of the situation is highlighted in the question 'Who is the surgeon to the boy?'\"}}, 'evaluation': {'evaluation': 'The result is a reasonable answer for the current subtask, as it accurately identifies the key elements in the query and asks a relevant question about the context.', 'retry': False}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': 'Consider the memory and the current subtask about understanding the relationship between the surgeon and the boy. Review the memory to find any mentions of relationships between the surgeon and the boy.'}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Consider the memory and the current subtask about understanding the relationship between the surgeon and the boy. Review the memory to find any mentions of relationships between the surgeon and the boy. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Understand the relationship between the surgeon and the boy to answer the question.', 'reasoning': 'Review the memory to find any mentions of relationships between the surgeon and the boy. Use this information to inform the reasoning and identify the potential relationship that could make the surgeon unable to operate.', 'action': {'action': 'reasoning', 'parameters': {'prompt': 'Consider the memory and the current subtask about understanding the relationship between the surgeon and the boy. Review the memory to find any mentions of relationships between the surgeon and the boy.'}}, 'evaluation': {'evaluation': \"The result is somewhat inconclusive as it only reviews the memory and doesn't provide any specific information about the relationship between the surgeon and the boy.\", 'retry': True}}\n",
            "\n",
            "\n",
            "Max retries reached for subtask: reasoning: Understand the relationship between the surgeon and the boy to answer the question.\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Review the memory for any direct statements about the surgeon's relationship with the boy, as stated in 'The surgeon, who is the boy's father, says.' Identify the applicable relationship type based on this statement.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Review the memory for any direct statements about the surgeon's relationship with the boy, as stated in 'The surgeon, who is the boy's father, says.' Identify the applicable relationship type based on this statement. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Determine the type of relationship that makes the surgeon unable to operate on the boy.', 'reasoning': \"Review the memory for any direct statements about the surgeon's relationship with the boy, as stated in 'The surgeon, who is the boy's father, says.' Identify the applicable relationship type based on this statement.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Review the memory for any direct statements about the surgeon's relationship with the boy, as stated in 'The surgeon, who is the boy's father, says.' Identify the applicable relationship type based on this statement.\"}}, 'evaluation': {'evaluation': 'The result is somewhat incomplete for the current subtask, as it does not provide a specific relationship type based on the relationship between the surgeon and the boy.', 'retry': True}}\n",
            "\n",
            "\n",
            "Max retries reached for subtask: reasoning: Determine the type of relationship that makes the surgeon unable to operate on the boy.\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Review the memory to further identify potential relationships that could make a surgeon unable to operate on a patient. Consider the relationship between the surgeon and the boy's family, professional regulations, and medical ethics codes.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Review the memory to further identify potential relationships that could make a surgeon unable to operate on a patient. Consider the relationship between the surgeon and the boy's family, professional regulations, and medical ethics codes. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.', 'reasoning': 'Consider various scenarios where a surgeon may be unable to operate on a patient, such as blood relations (e.g., parent, sibling, child), conflicts of interest, or professional regulations. Review medical ethics codes or general knowledge to strengthen the list of relationships.', 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Review the memory to further identify potential relationships that could make a surgeon unable to operate on a patient. Consider the relationship between the surgeon and the boy's family, professional regulations, and medical ethics codes.\"}}, 'evaluation': {'evaluation': 'The result is a reasonable answer for the current subtask because it correctly identifies potential relationships that could make a surgeon unable to operate on a patient.', 'retry': False}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Review the list of relationships from subtask 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.' and filter them to find the most relevant relationship that could make the surgeon unable to operate on the boy, given the statement 'The surgeon, who is the boy's father, says'. Then, use this relationship to answer the question 'Who is the surgeon to the boy?'\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Review the list of relationships from subtask 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.' and filter them to find the most relevant relationship that could make the surgeon unable to operate on the boy, given the statement 'The surgeon, who is the boy's father, says'. Then, use this relationship to answer the question 'Who is the surgeon to the boy?' ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Filter the list to find relationships that would be relevant to the question.', 'reasoning': \"Given the subtasks' results, particularly 'reasoning: Determine the type of relationship that makes the surgeon unable to operate on the boy.' and 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.', review the possibilities where the surgeon's relationship with the boy, such as being the boy's parent, is relevant to the question 'Who is the surgeon to the boy?'\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Review the list of relationships from subtask 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.' and filter them to find the most relevant relationship that could make the surgeon unable to operate on the boy, given the statement 'The surgeon, who is the boy's father, says'. Then, use this relationship to answer the question 'Who is the surgeon to the boy?'\"}}, 'evaluation': {'evaluation': 'The result is not a complete answer for the current subtask, as it does not filter the relationships to find the most relevant one that could make the surgeon unable to operate on the boy, given the provided context.', 'retry': True}}\n",
            "\n",
            "\n",
            "Max retries reached for subtask: reasoning: Filter the list to find relationships that would be relevant to the question.\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Based on the subtasks' results, particularly 'reasoning: Determine the type of relationship that makes the surgeon unable to operate on the boy.' and 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.', review the possibilities where the surgeon's relationship with the boy, such as being the boy's parent, is relevant to the question 'Who is the surgeon to the boy?'\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Based on the subtasks' results, particularly 'reasoning: Determine the type of relationship that makes the surgeon unable to operate on the boy.' and 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.', review the possibilities where the surgeon's relationship with the boy, such as being the boy's parent, is relevant to the question 'Who is the surgeon to the boy?' ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Select the most relevant relationship to answer the question.', 'reasoning': \"Given the statement 'The surgeon, who is the boy's father, says' and the question 'Who is the surgeon to the boy?', select the relationship where the surgeon is the boy's father as the most relevant relationship to answer the question, as it explains the surgeon's inability to operate on the boy due to a personal conflict of interest.\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Based on the subtasks' results, particularly 'reasoning: Determine the type of relationship that makes the surgeon unable to operate on the boy.' and 'reasoning: Generate a list of relationships that would make a surgeon unable to operate on a patient.', review the possibilities where the surgeon's relationship with the boy, such as being the boy's parent, is relevant to the question 'Who is the surgeon to the boy?'\"}}, 'evaluation': {'evaluation': 'The result is somewhat incomplete for the current subtask, as it does not filter the relationships to find the most relevant one that could make the surgeon unable to operate on the boy, given the provided context.', 'retry': True}}\n",
            "\n",
            "\n",
            "Max retries reached for subtask: reasoning: Select the most relevant relationship to answer the question.\n",
            "\n",
            "\n",
            " ****** Action Info: {'action': 'reasoning', 'parameters': {'prompt': \"Based on the relationship established that the surgeon is the boy's father, explain what this means for the surgeon's role in the boy's life and answer the question 'Who is the surgeon to the boy?' with this new understanding.\"}} ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ****** Execution Result: Based on the relationship established that the surgeon is the boy's father, explain what this means for the surgeon's role in the boy's life and answer the question 'Who is the surgeon to the boy?' with this new understanding. ****** \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STEP: {'subtask': 'reasoning: Use this relationship to determine who the surgeon is to the boy.', 'reasoning': \"Based on the relationship established that the surgeon is the boy's father, use this familial relationship to determine the surgeon's role in the boy's life. This connection indicates that the surgeon is the boy's parent, thus answering the question 'Who is the surgeon to the boy?'\", 'action': {'action': 'reasoning', 'parameters': {'prompt': \"Based on the relationship established that the surgeon is the boy's father, explain what this means for the surgeon's role in the boy's life and answer the question 'Who is the surgeon to the boy?' with this new understanding.\"}}, 'evaluation': {'evaluation': \"The result does not quite make sense in the context of the overall query because it contains a prompt from the previous subtask without resolving the current question of 'Who is the surgeon to the boy?' based on the established relationship that the surgeon is the boy's father.\", 'retry': True}}\n",
            "\n",
            "\n",
            "Max retries reached for subtask: reasoning: Use this relationship to determine who the surgeon is to the boy.\n",
            "The surgeon is the boy's father.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BR40zRarAA2t"
      }
    }
  ]
}